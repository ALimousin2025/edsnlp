{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Working with Spark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "AP-HP's clinical data warehouse uses Spark to distribute computations on a cluster. This example notebook shows how one can leverage PySpark to apply an NLP pipeline on a Spark DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Spark \"connector\" simply wraps the pipeline into a UDF and distributes it on a cluster."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Getting clinical notes\n",
    "\n",
    "This section supposes you have an PySpark `sql` object ready to use.\n",
    "If not, it can be created e.g. via\n",
    "\n",
    "```python\n",
    "from pyspark.sql.session import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sql = spark.sql\n",
    "```\n",
    "\n",
    "Now let us query some notes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "DB_NAME = 'edsomop_prod_a'\n",
    "TABLE_NAME = 'note'\n",
    "N = 1000\n",
    "\n",
    "notes = sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        person_id,\n",
    "        visit_occurrence_id,\n",
    "        note_id,\n",
    "        note_text\n",
    "    FROM\n",
    "        {DB_NAME}.{TABLE_NAME}\n",
    "    LIMIT {N}\"\"\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining an NLP pipeline\n",
    "\n",
    "Now, as you would do normally using EDS-NLP, let us define the pipeline whe want to apply on out notes.  \n",
    "In this example, we will construct a *dummy* `matcher`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Importating Spacy and all EDS-NLP pipes\n",
    "import spacy\n",
    "import edsnlp.components"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Creates the Spacy instance\n",
    "nlp = spacy.blank('fr')\n",
    "\n",
    "# Normalisation of accents, case and other special characters\n",
    "nlp.add_pipe('normalizer')\n",
    "# Detecting end of lines\n",
    "nlp.add_pipe('sentences')\n",
    "\n",
    "# Extraction of named entities\n",
    "nlp.add_pipe(\n",
    "    'matcher',\n",
    "    config=dict(\n",
    "        terms=dict(respiratoire=[\n",
    "            'difficultes respiratoires',\n",
    "            'asthmatique',\n",
    "            'toux',\n",
    "        ]),\n",
    "        regex=dict(\n",
    "            covid=r'(?i)(?:infection\\sau\\s)?(covid[\\s\\-]?19|corona[\\s\\-]?virus)',\n",
    "            traitement=r'(?i)traitements?|medicaments?',\n",
    "            respiratoire=\"respiratoires\",\n",
    "        ),\n",
    "        attr='NORM',\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Qualification of matched entities\n",
    "nlp.add_pipe('negation')\n",
    "nlp.add_pipe('hypothesis')\n",
    "nlp.add_pipe('family')\n",
    "nlp.add_pipe('rspeech')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<edsnlp.pipelines.rspeech.rspeech.ReportedSpeech at 0x7f9f98058d90>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applying the pipeline\n",
    "\n",
    "As shown above, we have defined a matcher which extracts entities, and added some qualifiers that add attributes to those entities.  \n",
    "Let us mention the used qualifiers here:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "qualifiers = ['negated', 'hypothesis', 'reported_speech', 'family']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, to apply the pipeline to the PySpark DataFrame, two options are available"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import pyspark.sql.functions as F\n",
    "from edsnlp.connectors.spark import udf_factory, apply_nlp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. The `udf_factory` function\n",
    "\n",
    "This function allows us to define a matcher:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "matcher = udf_factory(\n",
    "    nlp=nlp,\n",
    "    qualifiers=qualifiers,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now what's left to do is to apply this *matcher* on the DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Apply the matcher\n",
    "note_nlp = notes.withColumn(\"matches\", matcher(notes.note_text))\n",
    "\n",
    "# Formatting the output into separate columns\n",
    "note_nlp = note_nlp.withColumn(\"matches\", F.explode(note_nlp.matches))\n",
    "\n",
    "# Selection the columns of interest\n",
    "note_nlp = note_nlp.select(\"note_id\", \"matches.*\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's it, you now have a DataFrame containing one row per extracted entity, with the following columns:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "note_nlp.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['note_id',\n",
       " 'lexical_variant',\n",
       " 'label',\n",
       " 'discarded',\n",
       " 'start',\n",
       " 'end',\n",
       " 'negated',\n",
       " 'hypothesis',\n",
       " 'reported_speech',\n",
       " 'family']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. The `apply_nlp` function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "note_nlp = apply_nlp(notes, nlp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "note_nlp.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['note_nlp_id',\n",
       " 'note_id',\n",
       " 'lexical_variant',\n",
       " 'label',\n",
       " 'discarded',\n",
       " 'start',\n",
       " 'end',\n",
       " 'note_nlp_datetime']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Go deeper\n",
    "\n",
    "Out of the box, these two methods allows you to use relatively simple `spacy` pipes.\n",
    "However, you might want to\n",
    "- Extract spans stored as `SpanGroups` in the `doc.spans` dictionnary\n",
    "- Extract specific user-defined extensions (e.g. the `score_method` extension of the **SOFA** pipeline)\n",
    "For this, the two methods `udf_factory` and `apply_nlp` exposes 2 keyword arguments\n",
    "\n",
    "**The `additionnal_spans` argument**:  \n",
    "This argument can be either a string or a list of string. It tells the method to also retrieve `Spans` \n",
    "from the `doc.spans` dictionnary, using the provided key(s).\n",
    "\n",
    "**The `additionnal_extensions` argument**:  \n",
    "This argument should be a list of 2-Tuples. Each tuple is of the following structure:\n",
    "- The first element is a string designing the name of the extension, omitting the leading `_`.\n",
    "- The second element is a `pyspark.sql.types` type giving the type of the extension."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4c4e68f981817c04f42d58b62415293cfb1267b1fe1f33d5117402e7185407b"
  },
  "kernelspec": {
   "display_name": "[2.4.3] Py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
