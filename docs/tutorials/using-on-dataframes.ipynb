{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDS-NLP with Pandas or PySpark DataFrames\n",
    "\n",
    "The way EDS-NLP is used may depend on how many documents you are working with.  Once working with tens of thousands of them,\n",
    "parallelizing the processing can be really efficient (up to 8x faster), but will require a (tiny) bit more work.  \n",
    "Here are shown 4 ways to analyse texts depending on your needs:\n",
    "\n",
    "- [Testing / Using on a single string](#single_pipe)  \n",
    "\n",
    "- [Using on DataFrames](#df_pipes)\n",
    "\n",
    "   - [On a small Pandas DataFrame](#simple_pipe)\n",
    "   - [On a larger Pandas DataFrame](#parallel_pipe)\n",
    "   - [On a Spark DataFrame](#spark_pipe)\n",
    "\n",
    "A [wrapper](#wrapper) is available to simply switch between those use cases.\n",
    "\n",
    "Finally, you can check some [time benchmarks here](#timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"CRITICAL\")\n",
    "\n",
    "# One-shot import of all declared Spacy components\n",
    "import edsnlp.components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the necessary pipe functions for the sake of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edsnlp.processing import (\n",
    "    simple_pipe,\n",
    "    parallel_pipe,\n",
    "    spark_pipe,\n",
    "    pipe,\n",
    "    pyspark_type_finder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of testing, let us create a simple NLP pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"fr\")\n",
    "nlp.add_pipe(\"eds.sentences\")\n",
    "nlp.add_pipe(\"eds.normalizer\")\n",
    "\n",
    "regex = dict(\n",
    "    patient=[\n",
    "        \"patient\",\n",
    "        \"malade\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "nlp.add_pipe(\"eds.matcher\", config=dict(regex=regex, attr=\"NORM\"))\n",
    "nlp.add_pipe(\"eds.negation\")\n",
    "nlp.add_pipe(\"eds.hypothesis\")\n",
    "nlp.add_pipe(\"eds.family\")\n",
    "nlp.add_pipe(\"eds.dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"single_pipe\"></a>Pipeline on a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Patient admis le 25 Septembre 2021 pour suspicion de Covid.\n",
    "    Pas de cas de coronavirus dans ce service.\n",
    "    Le père du patient est atteind du covid.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply apply `nlp()` to the piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a quick look at what was extracted here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_ents_printer(doc):\n",
    "\n",
    "    headers = \"{:<25} {:<20} {:<30} {:<6} {:<6} {:<6}\"\n",
    "\n",
    "    print(headers.format(\"Text\", \"Label\", \"Span\", \"Neg\", \"Par\", \"Hyp\"))\n",
    "    for entite in doc.ents:\n",
    "        print(\n",
    "            headers.format(\n",
    "                entite.text,\n",
    "                entite.label_,\n",
    "                f\"({entite.start_char},{entite.end_char})\",\n",
    "                entite._.negation,\n",
    "                entite._.family,\n",
    "                entite._.hypothesis,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for date in doc.spans[\"dates\"]:\n",
    "        print(\n",
    "            headers.format(\n",
    "                date.text,\n",
    "                date.label_,\n",
    "                f\"({date.start_char},{date.end_char})\",\n",
    "                date._.negation,\n",
    "                date._.family,\n",
    "                date._.hypothesis,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text                      Label                Span                           Neg    Par    Hyp   \n",
      "Patient                   patient              (5,12)                         0      0      0     \n",
      "patient                   patient              (127,134)                      0      1      0     \n",
      "25 Septembre 2021         absolute             (22,39)                        0      0      0     \n"
     ]
    }
   ],
   "source": [
    "pretty_ents_printer(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"df_pipes\"></a>Pipelines on DataFrames\n",
    "\n",
    "We provide methods to use a SpaCy pipe directly on a Pandas or Spark DataFrame.\n",
    "Each method have a few arguments in common, namely:\n",
    "- `note`: a Pandas or Spark DataFrame containing at least a `note_text` and `note_id` columns\n",
    "- `nlp`: the spaCy pipeline\n",
    "- `additional_spans`: Each Spacy `Doc` has a `doc.spans` attribute, which is a dictionnary whose values are list of spans. For example, the `dates` pipe used above populates the `doc.spans['dates']` list. The `additional_spans` argument should be used to tell the function which lists you want to extract, in addition to the default `doc.ents` list.\n",
    "- `extensions`: By default, each method will extract the following informations from each `span`:\n",
    "   - \"note_id\": span.doc._.note_id\n",
    "   - \"lexical_variant\": span.text\n",
    "   - \"label\": span.label_\n",
    "   - \"start\": span.start_char\n",
    "   - \"end\": span.end_char  \n",
    "   \n",
    "Depending on your pipeline, you may want ot extract other extensions. To do so, simply provide those extension names (without the leading underscore) to the `extensions` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"simple_pipe\"></a>1. Pipeline on a small Pandas DataFrame\n",
    "\n",
    "We will here get documents from the cluster.  \n",
    "Depending on your acces, change the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"edsomop_prod_a\"\n",
    "TABLE_NAME = \"orbis_note\"\n",
    "NOTE_ID_COL = \"note_id\"\n",
    "NOTE_TEXT_COL = \"note_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_notes = sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        {NOTE_ID_COL} AS note_id,\n",
    "        {NOTE_TEXT_COL} AS note_text\n",
    "    FROM\n",
    "        {DB_NAME}.{TABLE_NAME}\n",
    "    WHERE\n",
    "        {NOTE_TEXT_COL} IS NOT NULL\n",
    "    LIMIT 100000\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = spark_notes.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us keep 1000 documents to make a small set of notes, and 10.000 for a medium subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_notes_subset = notes[:1000]\n",
    "medium_notes_subset = notes[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:49<00:00, 20.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.9 s, sys: 196 ms, total: 49.1 s\n",
      "Wall time: 49.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "note_nlp = simple_pipe(\n",
    "    small_notes_subset,\n",
    "    nlp,\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions=[\"parsed_date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>lexical_variant</th>\n",
       "      <th>label</th>\n",
       "      <th>span_type</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>parsed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>699317043</td>\n",
       "      <td>1/02/2017</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>220</td>\n",
       "      <td>229</td>\n",
       "      <td>2017-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>701135505</td>\n",
       "      <td>Patient</td>\n",
       "      <td>patient</td>\n",
       "      <td>ents</td>\n",
       "      <td>812</td>\n",
       "      <td>819</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>18060531806</td>\n",
       "      <td>03/06/2021</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>1248</td>\n",
       "      <td>1258</td>\n",
       "      <td>2021-06-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6062</th>\n",
       "      <td>19207919239</td>\n",
       "      <td>11/08/2021</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>14257</td>\n",
       "      <td>14267</td>\n",
       "      <td>2021-08-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7792</th>\n",
       "      <td>19747574107</td>\n",
       "      <td>23.02.21</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>3623</td>\n",
       "      <td>3631</td>\n",
       "      <td>2021-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          note_id lexical_variant     label span_type  start    end  \\\n",
       "7701    699317043       1/02/2017  absolute     dates    220    229   \n",
       "6229    701135505         Patient   patient      ents    812    819   \n",
       "3174  18060531806      03/06/2021  absolute     dates   1248   1258   \n",
       "6062  19207919239      11/08/2021  absolute     dates  14257  14267   \n",
       "7792  19747574107        23.02.21  absolute     dates   3623   3631   \n",
       "\n",
       "     parsed_date  \n",
       "7701  2017-02-01  \n",
       "6229         NaT  \n",
       "3174  2021-06-03  \n",
       "6062  2021-08-11  \n",
       "7792  2021-02-23  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_nlp.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"parallel_pipe\"></a>2. Pipeline on a larger Pandas DataFrame\n",
    "\n",
    "Here we will parallelise processes to speed up things "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n",
      "WARNING:root:[Parallel(n_jobs=-2)]: 100 tasks to complete\n",
      "[Parallel(n_jobs=-2)]: Using backend MultiprocessingBackend with 63 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done   8 out of 100 | elapsed:   13.2s remaining:  2.5min\n",
      "[Parallel(n_jobs=-2)]: Done  19 out of 100 | elapsed:   14.7s remaining:  1.0min\n",
      "[Parallel(n_jobs=-2)]: Done  30 out of 100 | elapsed:   15.7s remaining:   36.7s\n",
      "[Parallel(n_jobs=-2)]: Done  41 out of 100 | elapsed:   16.8s remaining:   24.2s\n",
      "[Parallel(n_jobs=-2)]: Done  52 out of 100 | elapsed:   18.3s remaining:   16.9s\n",
      "[Parallel(n_jobs=-2)]: Done  63 out of 100 | elapsed:   21.2s remaining:   12.5s\n",
      "[Parallel(n_jobs=-2)]: Done  74 out of 100 | elapsed:   24.2s remaining:    8.5s\n",
      "[Parallel(n_jobs=-2)]: Done  85 out of 100 | elapsed:   24.8s remaining:    4.4s\n",
      "[Parallel(n_jobs=-2)]: Done  96 out of 100 | elapsed:   26.0s remaining:    1.1s\n",
      "[Parallel(n_jobs=-2)]: Done 100 out of 100 | elapsed:   26.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 645 ms, sys: 1.9 s, total: 2.54 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "note_nlp = parallel_pipe(\n",
    "    medium_notes_subset,\n",
    "    nlp,\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions=[\"parsed_date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>lexical_variant</th>\n",
       "      <th>label</th>\n",
       "      <th>span_type</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>parsed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84970</th>\n",
       "      <td>18060754715</td>\n",
       "      <td>patiente</td>\n",
       "      <td>patient</td>\n",
       "      <td>ents</td>\n",
       "      <td>1955</td>\n",
       "      <td>1963</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84855</th>\n",
       "      <td>19916176337</td>\n",
       "      <td>16/09/2021</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>1559</td>\n",
       "      <td>1569</td>\n",
       "      <td>2021-09-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61236</th>\n",
       "      <td>8421417398</td>\n",
       "      <td>le mois</td>\n",
       "      <td>relative</td>\n",
       "      <td>dates</td>\n",
       "      <td>351</td>\n",
       "      <td>358</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9662</th>\n",
       "      <td>7884672495</td>\n",
       "      <td>18/02/2019</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>4131</td>\n",
       "      <td>4141</td>\n",
       "      <td>2019-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4749</th>\n",
       "      <td>697712250</td>\n",
       "      <td>26/12/2008</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>688</td>\n",
       "      <td>698</td>\n",
       "      <td>2008-12-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86068</th>\n",
       "      <td>16923880617</td>\n",
       "      <td>2018</td>\n",
       "      <td>year_only</td>\n",
       "      <td>dates</td>\n",
       "      <td>1687</td>\n",
       "      <td>1691</td>\n",
       "      <td>2018-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30925</th>\n",
       "      <td>9275339027</td>\n",
       "      <td>05/01/1960</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>1994</td>\n",
       "      <td>2004</td>\n",
       "      <td>1960-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49620</th>\n",
       "      <td>7067037758</td>\n",
       "      <td>01/2016</td>\n",
       "      <td>no_day</td>\n",
       "      <td>dates</td>\n",
       "      <td>615</td>\n",
       "      <td>622</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86535</th>\n",
       "      <td>18633073703</td>\n",
       "      <td>07/07/2021</td>\n",
       "      <td>absolute</td>\n",
       "      <td>dates</td>\n",
       "      <td>2053</td>\n",
       "      <td>2063</td>\n",
       "      <td>2021-07-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60954</th>\n",
       "      <td>18029401622</td>\n",
       "      <td>patient</td>\n",
       "      <td>patient</td>\n",
       "      <td>ents</td>\n",
       "      <td>3804</td>\n",
       "      <td>3811</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           note_id lexical_variant      label span_type  start   end  \\\n",
       "84970  18060754715        patiente    patient      ents   1955  1963   \n",
       "84855  19916176337      16/09/2021   absolute     dates   1559  1569   \n",
       "61236   8421417398         le mois   relative     dates    351   358   \n",
       "9662    7884672495      18/02/2019   absolute     dates   4131  4141   \n",
       "4749     697712250      26/12/2008   absolute     dates    688   698   \n",
       "86068  16923880617            2018  year_only     dates   1687  1691   \n",
       "30925   9275339027      05/01/1960   absolute     dates   1994  2004   \n",
       "49620   7067037758         01/2016     no_day     dates    615   622   \n",
       "86535  18633073703      07/07/2021   absolute     dates   2053  2063   \n",
       "60954  18029401622         patient    patient      ents   3804  3811   \n",
       "\n",
       "      parsed_date  \n",
       "84970         NaT  \n",
       "84855  2021-09-16  \n",
       "61236         NaT  \n",
       "9662   2019-02-18  \n",
       "4749   2008-12-26  \n",
       "86068  2018-01-01  \n",
       "30925  1960-01-05  \n",
       "49620  2016-01-01  \n",
       "86535  2021-07-07  \n",
       "60954         NaT  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "note_nlp.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"spark_pipe\"></a>3. Pipeline distributed on a Spark DataFrame\n",
    "\n",
    "Spark needs to know in advance the type of each extension you want to save.  \n",
    "Thus, if you need additional extensions to be saved, you will need to provide a dictionnary via the `extensions` argument, with the name of the extension as keys and its type as value.  \n",
    "Accepted types are the ones present in `pyspark.sql.types`.\n",
    "\n",
    "A helper, `pyspark_type_finder`, is available to get the correct type for most Python objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infered type is TimestampType\n"
     ]
    }
   ],
   "source": [
    "pyspark_datetime_type = pyspark_type_finder(datetime.datetime(2020, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------+---------+-----+----+-------------------+\n",
      "|    note_id|lexical_variant|   label|span_type|start| end|        parsed_date|\n",
      "+-----------+---------------+--------+---------+-----+----+-------------------+\n",
      "|16063715029|     21/12/1988|absolute|    dates| 1324|1334|1988-12-21 00:00:00|\n",
      "|16063715029|     21/01/2021|absolute|    dates| 1383|1393|2021-01-21 00:00:00|\n",
      "|16063715029|     21/01/2021|absolute|    dates| 1853|1863|2021-01-21 00:00:00|\n",
      "|16063715029|     21/12/1988|absolute|    dates| 1925|1935|1988-12-21 00:00:00|\n",
      "|16453909745|     16/10/1989|absolute|    dates| 1517|1527|1989-10-16 00:00:00|\n",
      "+-----------+---------------+--------+---------+-----+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 188 ms, sys: 23.8 ms, total: 212 ms\n",
      "Wall time: 6min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "note_nlp = spark_pipe(\n",
    "    spark_notes,\n",
    "    nlp,\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions={\"parsed_date\": pyspark_datetime_type},\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"wrapper\"></a>4. A wrapper for simpler usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `edsnlp.processing.pipe` wraps those 3 functions presented above in a single on.  \n",
    "It adds a `how` argument, which can be either `'simple'`, '`parallel'` or '`spark'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Small Pandas DataFrame\n",
    "\n",
    "note_nlp = pipe(\n",
    "    note=spark_notes.limit(1000).toPandas(),\n",
    "    nlp=nlp,\n",
    "    how=\"simple\",\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions=[\"parsed_date\"],\n",
    ")\n",
    "\n",
    "### Larger Pandas DataFrame\n",
    "\n",
    "note_nlp = pipe(\n",
    "    note=spark_notes.limit(10000).toPandas(),\n",
    "    nlp=nlp,\n",
    "    how=\"parallel\",\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions=[\"parsed_date\"],\n",
    ")\n",
    "\n",
    "### Small Pandas DataFrame\n",
    "\n",
    "note_nlp = pipe(\n",
    "    note=spark_notes,\n",
    "    nlp=nlp,\n",
    "    how=\"spark\",\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions={\"parsed_date\": pyspark_datetime_type},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"timing\"></a>Computationnal time comparison\n",
    "\n",
    "Let us compare the `simple` and `parallel` pipe on Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(note):\n",
    "    \"\"\"\n",
    "    Compare runtime between the two methods\n",
    "    \"\"\"\n",
    "    n = len(note)\n",
    "    t0 = time.time()\n",
    "\n",
    "    note_nlp = pipe(\n",
    "        note=note,\n",
    "        nlp=nlp,\n",
    "        how=\"simple\",\n",
    "        additional_spans=[\"dates\"],\n",
    "        extensions=[\"parsed_date\"],\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    t_simple = round(time.time() - t0)\n",
    "    note_nlp = pipe(\n",
    "        note=note,\n",
    "        nlp=nlp,\n",
    "        how=\"parallel\",\n",
    "        additional_spans=[\"dates\"],\n",
    "        extensions=[\"parsed_date\"],\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    t_parallel = round(time.time() - t0 - t_simple)\n",
    "    ratio = t_simple / t_parallel\n",
    "\n",
    "    speed_simple = round(60 * n / t_simple)\n",
    "    speed_parallel = round(60 * n / t_parallel)\n",
    "\n",
    "    t_simple = str(timedelta(seconds=t_simple))\n",
    "    t_parallel = str(timedelta(seconds=t_parallel))\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "For {n} documents:\n",
    "    Simple pipe took {t_simple} --> Mean of {speed_simple} docs/minute.\n",
    "    Parallel pipe took {t_parallel} --> Mean of {speed_parallel} docs/minute.\n",
    "    Parallel pipe is {round(ratio,2)} times faster than simple pipe\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_notes = [\n",
    "    notes[:100],\n",
    "    notes[:1000],\n",
    "    notes[:10000],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For 100 documents:\n",
      "    Simple pipe took 0:00:05 --> Mean of 1200 docs/minute.\n",
      "    Parallel pipe took 0:00:06 --> Mean of 1000 docs/minute.\n",
      "    Parallel pipe is 0.83 times faster than simple pipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For 1000 documents:\n",
      "    Simple pipe took 0:00:47 --> Mean of 1277 docs/minute.\n",
      "    Parallel pipe took 0:00:08 --> Mean of 7500 docs/minute.\n",
      "    Parallel pipe is 5.88 times faster than simple pipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For 10000 documents:\n",
      "    Simple pipe took 0:07:16 --> Mean of 1376 docs/minute.\n",
      "    Parallel pipe took 0:00:21 --> Mean of 28571 docs/minute.\n",
      "    Parallel pipe is 20.76 times faster than simple pipe\n"
     ]
    }
   ],
   "source": [
    "for notes_subset in list_notes:\n",
    "    process(notes_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the parallel method has some overhead with a few hundreds of documents, it gets way quicker with the number of inputs increasing.  \n",
    "It can run on the full 100.000 documents fairly quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n",
      "WARNING:root:[Parallel(n_jobs=-2)]: Used nlp components: ['eds.sentences', 'eds.normalizer', 'eds.matcher', 'eds.negation', 'eds.hypothesis', 'eds.family', 'eds.dates']\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = pipe(\n",
    "    note=notes,\n",
    "    nlp=nlp,\n",
    "    how=\"parallel\",\n",
    "    additional_spans=[\"dates\"],\n",
    "    extensions=[\"parsed_date\"],\n",
    "    progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27044ab3453e6340b72d28cacc194da199f9219fd79e2699a3e28fa401432bd9"
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "[2.4.3] Py3",
   "language": "python",
   "name": "pyspark-2.4.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
