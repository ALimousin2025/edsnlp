{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using and speeding-up EDS-NLP\n",
    "\n",
    "The way EDS-NLP is used may depend on how many documents you are working with.  Once working with tens of thousands of them,\n",
    "parallelizing the processing can be really efficient (up to 8x faster), but will require a (tiny) bit more work.  \n",
    "Here are shown 3 ways to analyse texts depending on your needs:\n",
    "\n",
    "- [Testing / Using on a single string](#1.-Pipeline-on-a-single-string)\n",
    "- [Using on a few documents](#2.-Pipeline-on-a-few-documents)\n",
    "- [Using on many documents](#3.-Pipeline-on-many-documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# One-shot import of all declared Spacy components\n",
    "import edsnlp.components\n",
    "\n",
    "# Module containing processing helpers\n",
    "import edsnlp.processing as nlprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('fr')\n",
    "nlp.add_pipe('sentences')\n",
    "nlp.add_pipe('normalizer')\n",
    "\n",
    "terms = dict(covid=[\"coronavirus\", \"covid19\", \"covid\"])\n",
    "\n",
    "nlp.add_pipe(\"matcher\", config=dict(terms=terms, attr=\"NORM\"))\n",
    "nlp.add_pipe('negation')\n",
    "nlp.add_pipe('hypothesis')\n",
    "nlp.add_pipe('family')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline on a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "    Patient admis pour suspicion de Covid.\n",
    "    Pas de cas de coronavirus dans ce service.\n",
    "    Le p√®re du patient est atteind du covid.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply apply `nlp()` to the piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a quick look at what was extracted here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_ents_printer(ents, limit=5):\n",
    "\n",
    "    headers = \"{:<15} {:<20} {:<30} {:<6} {:<6} {:<6}\"\n",
    "\n",
    "    print (headers.format('Text', 'Label','Span','Neg','Par','Hyp'))\n",
    "    for entite in ents[:limit]:\n",
    "        print(headers.format(entite.text,\n",
    "                             entite.label_,\n",
    "                             f\"({entite.start_char},{entite.end_char})\",\n",
    "                             entite._.negated,\n",
    "                             entite._.family,\n",
    "                             entite._.hypothesis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            Label                Span                           Neg    Par    Hyp   \n",
      "Covid           covid                (37,42)                        0      0      1     \n",
      "coronavirus     covid                (62,73)                        1      0      0     \n",
      "covid           covid                (129,134)                      0      1      0     \n"
     ]
    }
   ],
   "source": [
    "pretty_ents_printer(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pipeline on a few documents\n",
    "\n",
    "We will here get documents from the cluster. Depending on your acces, change the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = \"edsomop_prod_a\"\n",
    "TABLE_NAME = \"orbis_note\"\n",
    "NOTE_ID_COL = \"note_id\"\n",
    "NOTE_TEXT_COL = \"note_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = sql(\n",
    "    f\"\"\"\n",
    "    SELECT\n",
    "        {NOTE_ID_COL} AS note_id,\n",
    "        {NOTE_TEXT_COL} AS note_text\n",
    "    FROM\n",
    "        {DB_NAME}.{TABLE_NAME}\n",
    "    WHERE\n",
    "        {NOTE_TEXT_COL} IS NOT NULL\n",
    "    LIMIT 100000\n",
    "    \"\"\"\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us keep 1000 documents to make a small set of notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_notes_subset = notes[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `nlprocess.pipe` method (see its documentation for more details), we can directly give the DataFrame as input.  \n",
    "A SpaCy document will be created from each line.  \n",
    "If entities are extracted, we will store them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = []\n",
    "for doc in nlprocess.pipe(nlp,\n",
    "                          small_notes_subset,\n",
    "                          text_col='note_text',\n",
    "                          context_cols=['note_id'],\n",
    "                          progress_bar=False):\n",
    "    if len(doc.ents) > 0:\n",
    "        ents.extend(list(doc.ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text            Label                Span                           Neg    Par    Hyp   \n",
      "Covid           covid                (2722,2727)                    0      1      1     \n",
      "COVID           covid                (16865,16870)                  0      0      0     \n",
      "COVID           covid                (342,347)                      0      0      0     \n",
      "covid           covid                (4885,4890)                    0      0      1     \n",
      "Coronavirus     covid                (6610,6621)                    0      0      0     \n",
      "Coronavirus     covid                (9640,9651)                    0      0      0     \n",
      "coronavirus     covid                (9773,9784)                    1      0      0     \n",
      "COVID           covid                (1683,1688)                    1      0      0     \n",
      "COVID           covid                (1791,1796)                    0      0      0     \n",
      "COVID           covid                (2941,2946)                    0      0      0     \n",
      "COVID           covid                (3918,3923)                    0      0      0     \n",
      "Covid           covid                (14667,14672)                  0      0      0     \n",
      "coronavirus     covid                (1615,1626)                    0      0      0     \n",
      "Covid           covid                (2166,2171)                    0      0      0     \n",
      "Covid           covid                (1001,1006)                    0      0      0     \n"
     ]
    }
   ],
   "source": [
    "pretty_ents_printer(ents, limit=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pipeline on many documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go even faster, we have to **parallelize** the task.\n",
    "\n",
    "For more details, check the documentation in `Tutorials - Getting faster`\n",
    "\n",
    "To sum up what changes when parallelizing:\n",
    "1. The task is broken up into multiple processes.\n",
    "2. Each process saves intermediary results on memory.\n",
    "3. At the end, those results are aggregated and returned.\n",
    "\n",
    "The step 2. imposes that the intermediary results are **serializable**, i.e. we cannot simply save the SpaCy `Doc` object.  \n",
    "We need to tell the pipe what to save for each document: it is the goal of the `pick_results` function defined here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_results(doc):\n",
    "    \"\"\"\n",
    "    Function used well Paralellizing tasks via joblib\n",
    "    This functions will store all extracted entities\n",
    "    \"\"\"\n",
    "    return [{'note_id':e.doc._.note_id,\n",
    "             'lexical_variant':e.text,\n",
    "             'offset_start':e.start_char,\n",
    "             'offset_end':e.end_char,\n",
    "             'label':e.label_,\n",
    "             'negation':e._.negated,\n",
    "             'family':e._.family,\n",
    "             'hypothesis':e._.hypothesis} for e in doc.ents if doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can adjust this function however suits your needs the best.  \n",
    "\n",
    "Finally, the method `parallel_pipe` wraps everything up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = nlprocess.parallel_pipe(nlp,\n",
    "                               notes,\n",
    "                               chunksize=100,\n",
    "                               n_jobs=-2,\n",
    "                               context_cols='note_id',\n",
    "                               progress_bar=False,\n",
    "                               return_df=True,\n",
    "                               pick_results = pick_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>note_id</th>\n",
       "      <th>lexical_variant</th>\n",
       "      <th>offset_start</th>\n",
       "      <th>offset_end</th>\n",
       "      <th>label</th>\n",
       "      <th>negation</th>\n",
       "      <th>family</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10093568984</td>\n",
       "      <td>Covid</td>\n",
       "      <td>2722</td>\n",
       "      <td>2727</td>\n",
       "      <td>covid</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15157861124</td>\n",
       "      <td>COVID</td>\n",
       "      <td>16865</td>\n",
       "      <td>16870</td>\n",
       "      <td>covid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14747253531</td>\n",
       "      <td>COVID</td>\n",
       "      <td>342</td>\n",
       "      <td>347</td>\n",
       "      <td>covid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16420083409</td>\n",
       "      <td>covid</td>\n",
       "      <td>4885</td>\n",
       "      <td>4890</td>\n",
       "      <td>covid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16420083409</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>6610</td>\n",
       "      <td>6621</td>\n",
       "      <td>covid</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       note_id lexical_variant  offset_start  offset_end  label  negation  \\\n",
       "0  10093568984           Covid          2722        2727  covid     False   \n",
       "1  15157861124           COVID         16865       16870  covid     False   \n",
       "2  14747253531           COVID           342         347  covid     False   \n",
       "3  16420083409           covid          4885        4890  covid     False   \n",
       "4  16420083409     Coronavirus          6610        6621  covid     False   \n",
       "\n",
       "   family  hypothesis  \n",
       "0    True        True  \n",
       "1   False       False  \n",
       "2   False       False  \n",
       "3   False        True  \n",
       "4   False       False  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By giving the `note_id` into the `context_cols` argument, you can easily merge the results with your input DataFrame and keep on with your analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = ents.merge(notes, on='note_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time comparison\n",
    "\n",
    "Let us compare the last 2 methods on various number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(notes, method):\n",
    "    \"\"\"\n",
    "    Compare runtime between the two methods\n",
    "    \"\"\"\n",
    "    n = len(notes)\n",
    "    t0 = time.time()\n",
    "        \n",
    "    if method == \"Single process\":\n",
    "        \n",
    "        results = []\n",
    "        for doc in nlprocess.pipe(nlp,\n",
    "                                  notes,\n",
    "                                  text_col='note_text',\n",
    "                                  context_cols=['note_id'],\n",
    "                                  progress_bar=False):\n",
    "            if len(doc.ents) > 0:\n",
    "                results.extend(list(doc.ents))\n",
    "    \n",
    "    elif method == \"Parallel\":\n",
    "\n",
    "        results = nlprocess.parallel_pipe(nlp,\n",
    "                                          notes,\n",
    "                                          chunksize=100,\n",
    "                                          n_jobs=-2,\n",
    "                                          context_cols='note_id',\n",
    "                                          progress_bar=False,\n",
    "                                          return_df=True,\n",
    "                                          pick_results = pick_results)\n",
    "    \n",
    "    t1 = round(time.time() - t0)\n",
    "    str_time = str(timedelta(seconds=t1))\n",
    "    speed = round(60*n/t1)\n",
    "               \n",
    "    print(f\"{method}: Took {str_time} for {n} documents --> Mean of {speed} docs/minute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_notes = [\n",
    "    notes[:100],\n",
    "    notes[:1000],\n",
    "    notes[:10000]\n",
    "]\n",
    "\n",
    "list_methods = [\n",
    "    \"Single process\", # 2. Pipeline on a few documents\n",
    "    \"Parallel\"  # 3. Pipeline on many documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single process: Took 0:00:03 for 100 documents --> Mean of 2000 docs/minute\n",
      "Parallel: Took 0:00:04 for 100 documents --> Mean of 1500 docs/minute\n",
      "Single process: Took 0:00:20 for 1000 documents --> Mean of 3000 docs/minute\n",
      "Parallel: Took 0:00:04 for 1000 documents --> Mean of 15000 docs/minute\n",
      "Single process: Took 0:03:09 for 10000 documents --> Mean of 3175 docs/minute\n",
      "Parallel: Took 0:00:10 for 10000 documents --> Mean of 60000 docs/minute\n"
     ]
    }
   ],
   "source": [
    "for notes_subset in list_notes:\n",
    "    for method in list_methods:\n",
    "        process(notes_subset, method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the parallel method has some overhead with a few hundreds of documents, it gets way quicker with the number of inputs increasing.  \n",
    "It can run on the full 100.000 documents fairly quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel: Took 0:01:10 for 100000 documents --> Mean of 85714 docs/minute\n"
     ]
    }
   ],
   "source": [
    "process(notes, \"Parallel\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "[2.4.3] Py3",
   "language": "python",
   "name": "pyspark-2.4.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
